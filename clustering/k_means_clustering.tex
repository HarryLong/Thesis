\section{K-Means Clustering} \label{sec:clustering}

K-means is a well known and broadly used vector-quantization clustering algorithm \cite{Jain2010} which groups points into clusters based on the euclidean distance from a set of \textit{K} cluster means. It has been used to to tackle a wide range of problems including character recognition \cite{Perrone2000}, image segmentation \cite{Bradley1998,Kanungo2002} and compression \cite{Kanungo2002}.\\
The algorithm is of complexity O(N) and can be optimized to make use of parallel architectures \cite{Xu2005}. Efficiency is a key requirement for the terrain clustering process in order to provide interactive user feedback, irrespective of terrain size.\\
K-means often fails to produce adequate clusters when dealing with large dimensionality data sets \cite{Sun2012}. Because the data here has only four dimensions (slope, temperature, illumination and soil humidity), it is well-fitted. \\
Common downsides of K-means clustering, however, is the necessity to configure the number of clusters to produce \textit{K} and the selection of the initial \textit{K} points to act as the cluster seeds. How these are handled are discussed in sections \ref{subsec:n_cluster_conf} and \ref{subsec:init_clusters} respectively.\\

Given a set of data points \textit{P} and a configured value \textit{k}, k-means clustering behaves as follows:
\begin{enumerate}
\item Choose \textit{K} points at random to act as the seed cluster means, \textit{C$_{mean}$}.
\item Iterate through every data point \textit{P$_{n}$} from \textit{P} and calculate its Euclidean distance from each cluster mean using equation \ref{eq:cluster_dist_calc}. Point \textit{P} becomes a member of its closest cluster. 
\item Use the members of each cluster to calculate the new cluster means. 
\item Repeat from step 2. 
\end{enumerate}

\begin{equation} \label{eq:cluster_dist_calc}
D(A,B) = \sum_{n=1}^{m} (A_{n} - B_{n}) ^{2}
\end{equation}

Where: \textbf{A} and \textbf{B} are either data points or a cluster mean; \textbf{n} $\in$ \textbf{$\mathbb{R}_{m}$} is the dimensionality of the data set; \textbf{A$_{n}$} is the value of data point A in dimension \textit{n}.

\subsection{Algorithm Complexity}

For each clustering iteration, the algorithm needs to run through each data point twice. The first time, the distance between the data point and each mean cluster is calculated in order to determine the cluster membership.  The second time is to calculate the new cluster means. Given this, the algorithm complexity is $ 2 \times K \ times n \times i$ where: \textit{K} is the number of clusters, \textit{n} is the number of data points and \textit{i} is the total number of clustering iterations performed.

\subsection{Normalisation}

The Euclidean distance calculation outlined in equation \ref{eq:cluster_dist_calc} works well when all values have similar scale. Unfortunately, this is not the case for terrain resource data as illumination, slope, temperature and soil humidity are all measured in different units. This means that a one millimetre change in soil humidity will have as much of an influence on clustering as a one degree change in temperature. To equalise the effect on clustering across all resources, normalisation is performed based on the range of the individual resources. Equation \ref{eq:cluster_dist_calc_real} is used to calculate the Euclidean distance between two terrain positions (or cluster means) \textit{A} and \textit{B} for clustering.

\begin{equation} \label{eq:cluster_dist_calc_real}
D(A,B) = 
(\frac{\textit{S}(A) - \textit{S}(B)}{R_{S}})^{2} + 
\sum_{n=1}^{12}(
(\frac{\textit{T}_{n}(A) - \textit{T}_{n}(B)}{R_{T}}) ^{2} + 
(\frac{\textit{H}_{n}(A) - \textit{H}_{n}(B)}{R_{H}}) ^{2} + 
(\frac{\textit{I}_{n}(A) - \textit{I}_{n}(B)}{R_{I}}) ^{2} )
\end{equation}

Where:\textit{S(x)} is the slope of a point or cluster mean x; \textit{R$_{T}$(x)} is the slope range; ;\textit{T$_{n}$(x)} is the temperature of point or cluster mean x at month n; \textit{R$_{T}$(x)} is the temperature range; \textit{H$_{n}$(x)} is the soil humidity of point or cluster mean x at month n; \textit{R$_{H}$(x)} is the humidity range; \textit{I$_{n}$(x)} is the illumination of point or cluster mean x at month n; \textit{R$_{I}$(x)} is the illumination range; 

\subsection{Configuring the Number of Clusters, K} \label{subsec:n_cluster_conf}

The value of \textit{K} will affect the number of ecosystem simulators which need to be run when placing vegetation on the terrain. Although larger values will potentially result in more realistic vegetation distributions, the added simulations will require additional processing time. Choosing a good value for \textit{K} is therefore about finding a balance between realism and processing time and, as such, depends on user requirements. Too small of a K value will negatively impact realism as insufficient clusters will be generated and terrain vertices will be gathered into a same cluster even if they differ heavily in their underlying resources. Too big a value of K will negatively impact performance as ecosystem simulator runs will be performed to fill distinct areas of the terrain which are heavily similar in terms of the resources they offer. Because of this, the value of \textit{K} is required as input from the user before the clustering is performed.\\
Admitably, \textit{K} can be an abstract to users who don't have an understanding of K-means clustering. Future work could build on this and automatically calculate a suitable value of \textit{K} given the variance of the resources on the terrain.\\

\subsection{Choosing Seed Cluster Means} \label{subsec:init_clusters}

A downside of classic K-means clustering techniques is that clusters are non-reproducible. This is because the final clusters depend heavily on the initial seed points which were chosen to act as the cluster means. As these are selected at random, different runs will result in different clusters. Reproducibility is important here, however, as if the clusters cannot be reproduced neither will the final terrain. To ensure reproducibility, the terrain positions to act as the initial clusters are chosen using a pseudo-random number generator with a predefined and fixed seed.
